name: Enhanced Security Pipeline with AI Analysis
on: 
  push:
    branches: [main, master, develop]
  pull_request:
    branches: [main, master]

permissions:
  contents: read
  issues: write
  pull-requests: write
  actions: read

env:
  OLLAMA_HOST: ${{ secrets.OLLAMA_HOST || 'http://localhost:11434' }}
  MISTRAL_MODEL: mistral:7b-instruct-v0.2-q4_0

jobs:
  build:
    runs-on: ubuntu-latest
    name: Run unit tests and SAST scan on the source code 
    outputs:
      sonar-results: ${{ steps.sonar-scan.outputs.report-path }}
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up JDK 21
      uses: actions/setup-java@v4
      with:
        java-version: '21'
        distribution: 'zulu'
        cache: maven
    
    - name: Run Maven Tests
      run: mvn clean test
    
    - name: Run Sonar Analysis with cloud
      id: sonar-scan
      run: |
        mvn -B verify sonar:sonar \
          -Dsonar.projectKey=testtraining_testtraining \
          -Dsonar.organization=testtraining \
          -Dsonar.host.url=https://sonarcloud.io \
          -Dsonar.token=$SONAR_TOKEN \
          -Dsonar.ws.timeout=300
        
        # Download SonarCloud results
        sleep 30  # Wait for SonarCloud processing
        curl -u "$SONAR_TOKEN:" \
          "https://sonarcloud.io/api/issues/search?componentKeys=testtraining_testtraining&resolved=false" \
          -o sonar-results.json
        
        echo "report-path=sonar-results.json" >> $GITHUB_OUTPUT
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
    
    - name: Upload SonarQube Results
      uses: actions/upload-artifact@v4
      with:
        name: sonar-results
        path: sonar-results.json

  security:
    runs-on: ubuntu-latest
    needs: build
    name: Run the SCA scan on the source code
    outputs:
      snyk-results: ${{ steps.snyk-scan.outputs.report-path }}
    steps:
      - uses: actions/checkout@master
      
      - name: Run Snyk to check for vulnerabilities
        id: snyk-scan
        uses: snyk/actions/maven@master
        continue-on-error: true
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          args: --json-file-output=snyk-results.json
      
      - name: Upload Snyk Results
        uses: actions/upload-artifact@v4
        with:
          name: snyk-results
          path: snyk-results.json

  zap_scan:
    runs-on: ubuntu-latest
    needs: security
    name: Run DAST scan on the web application
    outputs:
      zap-results: ${{ steps.zap-scan.outputs.report-path }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: master
      
      - name: ZAP Scan
        id: zap-scan
        uses: zaproxy/action-baseline@v0.14.0
        with:
          target: 'http://testphp.vulnweb.com/'
          rules_file_name: '.zap/rules.tsv'
          cmd_options: '-a -J zap-report.json'
      
      - name: Upload ZAP Results
        uses: actions/upload-artifact@v4
        with:
          name: zap-results
          path: zap-report.json

  # NEW: AI-Powered Security Analysis
  ai_security_analysis:
    runs-on: ubuntu-latest
    needs: [build, security, zap_scan]
    name: AI-Powered Security Intelligence Analysis
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests python-dotenv
      
      - name: Download all scan results
        uses: actions/download-artifact@v4
        with:
          path: scan-results/
      
      - name: Setup Ollama (if not using external server)
        if: ${{ !secrets.OLLAMA_HOST }}
        run: |
          # Install Ollama
          curl -fsSL https://ollama.ai/install.sh | sh
          
          # Start Ollama in background
          ollama serve &
          sleep 10
          
          # Pull Mistral model
          ollama pull mistral:7b-instruct-v0.2-q4_0
          
          # Verify installation
          ollama list
      
      - name: Create Security Context
        run: |
          cat > security-context.json << EOF
          {
            "business_criticality": "high",
            "compliance_requirements": ["SOC2", "OWASP"],
            "attack_surface": ["web_application", "api_endpoints"],
            "environment": "production",
            "data_classification": "confidential",
            "risk_tolerance": "low",
            "remediation_sla": {
              "critical": "24_hours",
              "high": "72_hours",
              "medium": "7_days",
              "low": "30_days"
            },
            "critical_assets": [
              "authentication_system",
              "user_database",
              "payment_processing"
            ]
          }
          EOF
      
      - name: Create AI Security Agent
        run: |
          cat > ai_security_agent.py << 'EOF'
          import json
          import requests
          import sys
          import os
          from typing import Dict, List, Any
          from dataclasses import dataclass
          import time

          @dataclass
          class SecurityFinding:
              tool: str
              severity: str
              vulnerability: str
              file_path: str
              line_number: int
              description: str
              cwe_id: str
              cvss_score: float
              raw_data: dict

          class OllamaMistralAgent:
              def __init__(self, host="http://localhost:11434", model="mistral:7b-instruct-v0.2-q4_0"):
                  self.host = host
                  self.model = model
                  self.context = self.load_security_context()
              
              def load_security_context(self):
                  try:
                      with open('security-context.json', 'r') as f:
                          return json.load(f)
                  except FileNotFoundError:
                      return {"business_criticality": "medium"}
              
              def query_mistral(self, prompt: str, max_retries=3) -> str:
                  for attempt in range(max_retries):
                      try:
                          payload = {
                              "model": self.model,
                              "prompt": prompt,
                              "stream": False,
                              "options": {
                                  "temperature": 0.1,
                                  "num_ctx": 4096,
                                  "num_predict": 2048
                              }
                          }
                          
                          response = requests.post(f"{self.host}/api/generate", json=payload, timeout=300)
                          response.raise_for_status()
                          return response.json().get("response", "")
                      except Exception as e:
                          print(f"Attempt {attempt + 1} failed: {e}")
                          if attempt < max_retries - 1:
                              time.sleep(5)
                          else:
                              return f"Error: Unable to query Mistral after {max_retries} attempts"
              
              def analyze_cross_tool_correlation(self, findings: List[SecurityFinding]) -> Dict:
                  findings_summary = []
                  for f in findings[:20]:  # Limit to avoid token limits
                      findings_summary.append({
                          "tool": f.tool,
                          "severity": f.severity,
                          "vulnerability": f.vulnerability,
                          "file": f.file_path,
                          "description": f.description[:200],  # Truncate long descriptions
                          "cwe": f.cwe_id,
                          "cvss": f.cvss_score
                      })
                  
                  prompt = f"""
          As a cybersecurity expert, analyze these security findings from multiple tools:

          Business Context:
          - Criticality: {self.context.get('business_criticality', 'medium')}
          - Compliance: {', '.join(self.context.get('compliance_requirements', []))}
          - Risk Tolerance: {self.context.get('risk_tolerance', 'medium')}

          Security Findings ({len(findings)} total, showing top 20):
          {json.dumps(findings_summary, indent=2)}

          Provide a JSON analysis with these keys:
          1. "risk_summary": Overall risk assessment
          2. "top_priorities": Top 5 findings with business impact scores (1-10)
          3. "correlations": Findings that appear related or from same root cause
          4. "false_positive_candidates": Likely false positives with reasoning
          5. "attack_chains": Potential attack sequences using multiple vulnerabilities
          6. "remediation_sequence": Recommended fix order with justification

          Respond ONLY with valid JSON, no additional text.
                  """
                  
                  response = self.query_mistral(prompt)
                  try:
                      # Try to extract JSON from response
                      start = response.find('{')
                      end = response.rfind('}') + 1
                      if start >= 0 and end > start:
                          return json.loads(response[start:end])
                  except:
                      pass
                  
                  # Fallback response
                  return {
                      "risk_summary": "AI analysis failed, manual review required",
                      "top_priorities": [],
                      "correlations": [],
                      "false_positive_candidates": [],
                      "attack_chains": [],
                      "remediation_sequence": [],
                      "raw_ai_response": response
                  }

          def parse_scan_results():
              findings = []
              
              # Parse SonarQube results
              try:
                  with open('scan-results/sonar-results/sonar-results.json', 'r') as f:
                      sonar_data = json.load(f)
                      for issue in sonar_data.get('issues', []):
                          findings.append(SecurityFinding(
                              tool="SonarQube",
                              severity=issue.get('severity', 'UNKNOWN'),
                              vulnerability=issue.get('rule', 'Unknown Rule'),
                              file_path=issue.get('component', '').replace('testtraining_testtraining:', ''),
                              line_number=issue.get('line', 0),
                              description=issue.get('message', ''),
                              cwe_id=issue.get('tags', [''])[0] if issue.get('tags') else '',
                              cvss_score=0.0,
                              raw_data=issue
                          ))
              except Exception as e:
                  print(f"Error parsing SonarQube results: {e}")
              
              # Parse Snyk results
              try:
                  with open('scan-results/snyk-results/snyk-results.json', 'r') as f:
                      snyk_data = json.load(f)
                      for vuln in snyk_data.get('vulnerabilities', []):
                          findings.append(SecurityFinding(
                              tool="Snyk",
                              severity=vuln.get('severity', 'unknown'),
                              vulnerability=vuln.get('title', 'Unknown Vulnerability'),
                              file_path=vuln.get('from', [''])[0] if vuln.get('from') else '',
                              line_number=0,
                              description=vuln.get('description', ''),
                              cwe_id=vuln.get('identifiers', {}).get('CWE', [''])[0] if vuln.get('identifiers', {}).get('CWE') else '',
                              cvss_score=vuln.get('cvssScore', 0.0),
                              raw_data=vuln
                          ))
              except Exception as e:
                  print(f"Error parsing Snyk results: {e}")
              
              # Parse ZAP results
              try:
                  with open('scan-results/zap-results/zap-report.json', 'r') as f:
                      zap_data = json.load(f)
                      site_data = zap_data.get('site', [{}])[0] if zap_data.get('site') else {}
                      for alert in site_data.get('alerts', []):
                          findings.append(SecurityFinding(
                              tool="ZAP",
                              severity=alert.get('riskdesc', 'Unknown Risk'),
                              vulnerability=alert.get('name', 'Unknown Alert'),
                              file_path=alert.get('url', ''),
                              line_number=0,
                              description=alert.get('desc', ''),
                              cwe_id=str(alert.get('cweid', '')),
                              cvss_score=0.0,
                              raw_data=alert
                          ))
              except Exception as e:
                  print(f"Error parsing ZAP results: {e}")
              
              return findings

          def main():
              print("Starting AI Security Analysis...")
              
              # Parse all security findings
              findings = parse_scan_results()
              print(f"Found {len(findings)} total security findings")
              
              if not findings:
                  print("No security findings to analyze")
                  return
              
              # Initialize AI agent
              agent = OllamaMistralAgent()
              
              # Run AI analysis
              print("Running AI correlation analysis...")
              analysis = agent.analyze_cross_tool_correlation(findings)
              
              # Save results
              final_results = {
                  "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
                  "total_findings": len(findings),
                  "tools_used": list(set(f.tool for f in findings)),
                  "ai_analysis": analysis,
                  "findings_by_severity": {
                      "critical": len([f for f in findings if 'critical' in f.severity.lower()]),
                      "high": len([f for f in findings if 'high' in f.severity.lower()]),
                      "medium": len([f for f in findings if 'medium' in f.severity.lower()]),
                      "low": len([f for f in findings if 'low' in f.severity.lower()])
                  }
              }
              
              with open('ai-analysis-results.json', 'w') as f:
                  json.dump(final_results, f, indent=2)
              
              print("AI Security Analysis Complete!")
              print(f"Results saved to ai-analysis-results.json")
              
              # Print summary
              print("\n=== AI ANALYSIS SUMMARY ===")
              print(f"Risk Assessment: {analysis.get('risk_summary', 'Unknown')}")
              print(f"Top Priorities: {len(analysis.get('top_priorities', []))}")
              print(f"Potential Correlations: {len(analysis.get('correlations', []))}")

          if __name__ == "__main__":
              main()
          EOF
      
      - name: Run AI Security Analysis
        run: |
          python ai_security_agent.py
        env:
          OLLAMA_HOST: ${{ env.OLLAMA_HOST }}
      
      - name: Generate Executive Summary
        run: |
          cat > generate_summary.py << 'EOF'
          import json
          from datetime import datetime

          def generate_executive_report():
              try:
                  with open('ai-analysis-results.json', 'r') as f:
                      results = json.load(f)
              except:
                  return "# Security Analysis Report\n\nError: Could not load analysis results"
              
              report = f"""# Security Analysis Executive Summary

          **Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
          **Pipeline Run:** ${{{{ github.run_number }}}}

          ## ðŸ“Š Key Metrics
          - **Total Findings:** {results.get('total_findings', 0)}
          - **Tools Used:** {', '.join(results.get('tools_used', []))}
          - **Critical:** {results.get('findings_by_severity', {}).get('critical', 0)}
          - **High:** {results.get('findings_by_severity', {}).get('high', 0)}
          - **Medium:** {results.get('findings_by_severity', {}).get('medium', 0)}

          ## ðŸ¤– AI-Powered Analysis
          **Risk Assessment:** {results.get('ai_analysis', {}).get('risk_summary', 'Analysis pending')}

          ### ðŸŽ¯ Top Priority Issues
          """
              
              priorities = results.get('ai_analysis', {}).get('top_priorities', [])
              for i, priority in enumerate(priorities[:5], 1):
                  if isinstance(priority, dict):
                      report += f"{i}. **{priority.get('vulnerability', 'Unknown')}** (Impact: {priority.get('business_impact', 'N/A')})\n"
                  else:
                      report += f"{i}. {priority}\n"
              
              correlations = results.get('ai_analysis', {}).get('correlations', [])
              if correlations:
                  report += f"\n### ðŸ”— Security Correlations\n"
                  for correlation in correlations[:3]:
                      report += f"- {correlation}\n"
              
              remediation = results.get('ai_analysis', {}).get('remediation_sequence', [])
              if remediation:
                  report += f"\n### ðŸ› ï¸ Recommended Actions\n"
                  for i, action in enumerate(remediation[:3], 1):
                      report += f"{i}. {action}\n"
              
              return report

          with open('executive-summary.md', 'w') as f:
              f.write(generate_executive_report())
              
          print("Executive summary generated")
          EOF
          
          python generate_summary.py
      
      - name: Upload AI Analysis Results
        uses: actions/upload-artifact@v4
        with:
          name: ai-analysis-results
          path: |
            ai-analysis-results.json
            executive-summary.md
      
      - name: Comment on PR with AI Analysis
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              const summary = fs.readFileSync('executive-summary.md', 'utf8');
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## ðŸ¤– AI Security Analysis Results\n\n${summary}`
              });
            } catch (error) {
              console.log('Error posting comment:', error);
            }

  # NEW: Automated Remediation (Optional)
  auto_remediation:
    runs-on: ubuntu-latest
    needs: ai_security_analysis
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    name: Automated Security Remediation
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Download AI Analysis
        uses: actions/download-artifact@v4
        with:
          name: ai-analysis-results
      
      - name: Create Security Issues
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              const results = JSON.parse(fs.readFileSync('ai-analysis-results.json', 'utf8'));
              const priorities = results.ai_analysis?.top_priorities || [];
              
              for (const priority of priorities.slice(0, 3)) {
                if (typeof priority === 'object' && priority.vulnerability) {
                  await github.rest.issues.create({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    title: `ðŸ”’ Security: ${priority.vulnerability}`,
                    body: `## AI-Identified Security Issue
                    
          **Severity:** ${priority.severity || 'Unknown'}
          **Business Impact:** ${priority.business_impact || 'Unknown'}
          **Tool:** ${priority.tool || 'Multiple'}
          
          **Description:**
          ${priority.description || 'AI-identified security vulnerability requiring attention'}
          
          **Recommended Action:**
          ${priority.recommended_action || 'Manual review and remediation required'}
          
          ---
          *This issue was automatically created by AI security analysis*`,
                    labels: ['security', 'ai-detected', priority.severity?.toLowerCase() || 'medium']
                  });
                }
              }
            } catch (error) {
              console.log('Error creating issues:', error);
            }